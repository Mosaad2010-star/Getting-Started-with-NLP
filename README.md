 # üß† NLP Preprocessing for Embeddings

This guide explains how to properly preprocess text data when using different types of word embeddings in Natural Language Processing (NLP), including **GloVe**, **Word2Vec**, and **BERT**.

---

## üìå Table of Contents

1. [Static Embeddings (GloVe, Word2Vec)](#1-static-embeddings-glove-word2vec)
2. [Contextual Embeddings (BERT, RoBERTa)](#2-contextual-embeddings-bert-roberta)
3. [Summary Table](#3-summary-table)
4. [Python Examples](#4-python-examples)
5. [Tips](#5-tips)

---

## 1. Static Embeddings (GloVe, Word2Vec)

Static embeddings map each word to a fixed vector, regardless of context.

### ‚úÖ Preprocessing Steps:

- ‚úÖ Convert text to **lowercase**
- ‚úÖ **Remove punctuation** and non-alphabetic characters (optional)
- ‚úÖ **Tokenize** the text
- ‚úÖ **Remove stopwords** (optional)
- ‚úÖ **Lemmatize or stem** words (optional)
- ‚úÖ **Map tokens to vectors** using a pre-trained embedding

> üì¶ Use with models like: LSTM, GRU, traditional ML

---

## 2. Contextual Embeddings (BERT, RoBERTa)

These embeddings generate **context-aware** representations for each word/token.

### üö´ Do NOT:
- ‚ùå Lowercase manually (use correct tokenizer/model)
- ‚ùå Remove punctuation or stopwords
- ‚ùå Tokenize with external tools

### ‚úÖ Instead:
- ‚úÖ Use the **tokenizer provided by the model**
- ‚úÖ Ensure correct casing (`bert-base-cased` vs `uncased`)
- ‚úÖ Include special tokens (`[CLS]`, `[SEP]`)
- ‚úÖ Return attention masks and token types if needed

> üì¶ Use with transformer-based models like: BERT, RoBERTa, DistilBERT

---

## 3. Summary Table

| Step                     | Static Embeddings (GloVe/Word2Vec) | Contextual Embeddings (BERT) |
|--------------------------|------------------------------------|------------------------------|
| Lowercasing              | ‚úÖ Yes                              | ‚úÖ if `uncased`, ‚ùå if `cased` |
| Remove punctuation       | ‚úÖ Often                            | ‚ùå No                         |
| Remove stopwords         | ‚úÖ Optional                         | ‚ùå No                         |
| Tokenization             | ‚úÖ Manual (e.g., `nltk`)            | ‚úÖ Use model tokenizer        |
| Lemmatization/Stemming   | ‚úÖ Optional                         | ‚ùå No                         |
| Embedding lookup         | ‚úÖ One vector per word              | ‚úÖ Contextualized per token   |

---

## 4. Python Examples

### üìå GloVe Preprocessing Example:

```python
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
import string

nltk.download('punkt')
nltk.download('stopwords')

text = "The quick brown fox jumps over the lazy dog."
text = text.lower()
tokens = word_tokenize(text)
tokens = [t for t in tokens if t not in string.punctuation]
tokens = [t for t in tokens if t not in stopwords.words('english')]
print(tokens)

